{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"doubtlab DoubtLab helps you find bad labels. This repository contains general tricks that may help you find bad, or noisy, labels in your dataset. The hope is that this repository makes it easier for folks to quickly check their own datasets before they invest too much time and compute on gridsearch. Installation \u00b6 You can install the tool via pip or conda . Install with pip python -m pip install doubtlab Install with conda conda install -c conda-forge doubtlab Getting Started \u00b6 If you want to get started, we recommend starting here . Related Projects \u00b6 The cleanlab project was an inspiration for this one. They have a great heuristic for bad label detection but I wanted to have a library that implements many. Be sure to check out their work on the labelerrors.com project. My employer, Rasa , has always had a focus on data quality. Some of that attitude is bound to have seeped in here. Be sure to check the Conversation Driven Development approach and Rasa X if you're working on virtual assistants.","title":"Home"},{"location":"#installation","text":"You can install the tool via pip or conda . Install with pip python -m pip install doubtlab Install with conda conda install -c conda-forge doubtlab","title":"Installation"},{"location":"#getting-started","text":"If you want to get started, we recommend starting here .","title":"Getting Started"},{"location":"#related-projects","text":"The cleanlab project was an inspiration for this one. They have a great heuristic for bad label detection but I wanted to have a library that implements many. Be sure to check out their work on the labelerrors.com project. My employer, Rasa , has always had a focus on data quality. Some of that attitude is bound to have seeped in here. Be sure to check the Conversation Driven Development approach and Rasa X if you're working on virtual assistants.","title":"Related Projects"},{"location":"api/doubtlab/","text":"DoubtEnsemble \u00b6 A pipeline to find bad labels. Parameters Name Type Description Default **reasons kwargs with (name, reason)-pairs {} Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason , WrongPredictionReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) reasons = { \"proba\" : ProbaReason ( model = model ), \"wrong_pred\" : WrongPredictionReason ( model = model ), } doubt = DoubtEnsemble ( ** reasons ) get_indices ( self , X , y = None ) \u00b6 Show source code in doubtlab/ensemble.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def get_indices ( self , X , y = None ): \"\"\" Calculates indices worth checking again. Arguments: X: the `X` data to be processed y: the `y` data to be processed Usage: ```python from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason, WrongPredictionReason X, y = load_iris(return_X_y=True) model = LogisticRegression(max_iter=1_000) model.fit(X, y) reasons = { \"proba\": ProbaReason(model=model), \"wrong_pred\": WrongPredictionReason(model=model), } doubt = DoubtEnsemble(**reasons) indices = doubt.get_indices(X, y) ``` \"\"\" df = self . get_predicates ( X , y ) predicates = [ c for c in df . columns if isinstance ( c , str ) and ( \"predicate\" in c ) ] return np . array ( [ int ( i ) for i in df . loc [ lambda d : d [ predicates ] . sum ( axis = 1 ) > 0 ] . index ] ) Calculates indices worth checking again. Parameters Name Type Description Default X the X data to be processed required y the y data to be processed None Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason , WrongPredictionReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) reasons = { \"proba\" : ProbaReason ( model = model ), \"wrong_pred\" : WrongPredictionReason ( model = model ), } doubt = DoubtEnsemble ( ** reasons ) indices = doubt . get_indices ( X , y ) get_predicates ( self , X , y = None ) \u00b6 Show source code in doubtlab/ensemble.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def get_predicates ( self , X , y = None ): \"\"\" Returns a sorted dataframe that shows the reasoning behind the sorting. Arguments: X: the `X` data to be processed y: the `y` data to be processed Usage: ```python from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason, WrongPredictionReason X, y = load_iris(return_X_y=True) model = LogisticRegression(max_iter=1_000) model.fit(X, y) reasons = { \"proba\": ProbaReason(model=model), \"wrong_pred\": WrongPredictionReason(model=model), } doubt = DoubtEnsemble(**reasons) predicates = doubt.get_predicates(X, y) ``` \"\"\" df = pd . DataFrame ( { f \"predicate_ { name } \" : func ( X , y ) for name , func in self . reasons . items ()} ) sorted_index = df . sum ( axis = 1 ) . sort_values ( ascending = False ) . index return df . reindex ( sorted_index ) Returns a sorted dataframe that shows the reasoning behind the sorting. Parameters Name Type Description Default X the X data to be processed required y the y data to be processed None Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason , WrongPredictionReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) reasons = { \"proba\" : ProbaReason ( model = model ), \"wrong_pred\" : WrongPredictionReason ( model = model ), } doubt = DoubtEnsemble ( ** reasons ) predicates = doubt . get_predicates ( X , y )","title":"ensemble"},{"location":"api/doubtlab/#doubtensemble","text":"A pipeline to find bad labels. Parameters Name Type Description Default **reasons kwargs with (name, reason)-pairs {} Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason , WrongPredictionReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) reasons = { \"proba\" : ProbaReason ( model = model ), \"wrong_pred\" : WrongPredictionReason ( model = model ), } doubt = DoubtEnsemble ( ** reasons )","title":"DoubtEnsemble"},{"location":"api/doubtlab/#doubtlab.ensemble.DoubtEnsemble.get_indices","text":"Show source code in doubtlab/ensemble.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def get_indices ( self , X , y = None ): \"\"\" Calculates indices worth checking again. Arguments: X: the `X` data to be processed y: the `y` data to be processed Usage: ```python from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason, WrongPredictionReason X, y = load_iris(return_X_y=True) model = LogisticRegression(max_iter=1_000) model.fit(X, y) reasons = { \"proba\": ProbaReason(model=model), \"wrong_pred\": WrongPredictionReason(model=model), } doubt = DoubtEnsemble(**reasons) indices = doubt.get_indices(X, y) ``` \"\"\" df = self . get_predicates ( X , y ) predicates = [ c for c in df . columns if isinstance ( c , str ) and ( \"predicate\" in c ) ] return np . array ( [ int ( i ) for i in df . loc [ lambda d : d [ predicates ] . sum ( axis = 1 ) > 0 ] . index ] ) Calculates indices worth checking again. Parameters Name Type Description Default X the X data to be processed required y the y data to be processed None Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason , WrongPredictionReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) reasons = { \"proba\" : ProbaReason ( model = model ), \"wrong_pred\" : WrongPredictionReason ( model = model ), } doubt = DoubtEnsemble ( ** reasons ) indices = doubt . get_indices ( X , y )","title":"get_indices()"},{"location":"api/doubtlab/#doubtlab.ensemble.DoubtEnsemble.get_predicates","text":"Show source code in doubtlab/ensemble.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def get_predicates ( self , X , y = None ): \"\"\" Returns a sorted dataframe that shows the reasoning behind the sorting. Arguments: X: the `X` data to be processed y: the `y` data to be processed Usage: ```python from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason, WrongPredictionReason X, y = load_iris(return_X_y=True) model = LogisticRegression(max_iter=1_000) model.fit(X, y) reasons = { \"proba\": ProbaReason(model=model), \"wrong_pred\": WrongPredictionReason(model=model), } doubt = DoubtEnsemble(**reasons) predicates = doubt.get_predicates(X, y) ``` \"\"\" df = pd . DataFrame ( { f \"predicate_ { name } \" : func ( X , y ) for name , func in self . reasons . items ()} ) sorted_index = df . sum ( axis = 1 ) . sort_values ( ascending = False ) . index return df . reindex ( sorted_index ) Returns a sorted dataframe that shows the reasoning behind the sorting. Parameters Name Type Description Default X the X data to be processed required y the y data to be processed None Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason , WrongPredictionReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) reasons = { \"proba\" : ProbaReason ( model = model ), \"wrong_pred\" : WrongPredictionReason ( model = model ), } doubt = DoubtEnsemble ( ** reasons ) predicates = doubt . get_predicates ( X , y )","title":"get_predicates()"},{"location":"api/reasons/","text":"from doubtlab.reason import * \u00b6 AbsoluteDifferenceReason \u00b6 Assign doubt when the absolute difference between label and regression is too large. Parameters Name Type Description Default model scikit-learn outlier model required threshold cutoff for doubt assignment required Usage: from sklearn.datasets import load_diabetes from sklearn.linear_model import LinearRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import AbsoluteDifferenceReason X , y = load_diabetes ( return_X_y = True ) model = LinearRegression () model . fit ( X , y ) doubt = DoubtEnsemble ( reason = AbsoluteDifferenceReason ( model , threshold = 100 )) indices = doubt . get_indices ( X , y ) CleanlabReason \u00b6 Assign doubt when using the cleanlab heuristic. Parameters Name Type Description Default model scikit-learn outlier model required sorted_index_method method used by cleanlab for sorting indices 'normalized_margin' min_doubt the minimum doubt output value used for sorting by the ensemble 0.5 Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import CleanlabReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression () model . fit ( X , y ) doubt = DoubtEnsemble ( reason = CleanlabReason ( model )) indices = doubt . get_indices ( X , y ) from_proba ( proba , y , min_doubt = 0.5 , sorted_index_method = 'normalized_margin' ) (staticmethod) \u00b6 Show source code in doubtlab/reason.py 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 @staticmethod def from_proba ( proba , y , min_doubt = 0.5 , sorted_index_method = \"normalized_margin\" ): \"\"\" Outputs a reason array from a proba array, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import CleanlabReason probas = np.array([[0.9, 0.1], [0.5, 0.5]]) y = np.array([0, 1]) predicate = CleanlabReason.from_proba(probas, y) ``` \"\"\" ordered_label_errors = get_noise_indices ( y , proba , sorted_index_method ) result = np . zeros_like ( y ) conf_arr = np . linspace ( 1 , min_doubt , result . shape [ 0 ]) for idx , _ in zip ( ordered_label_errors , conf_arr ): result [ idx ] = 1 return result . astype ( np . float16 ) Outputs a reason array from a proba array, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import CleanlabReason probas = np . array ([[ 0.9 , 0.1 ], [ 0.5 , 0.5 ]]) y = np . array ([ 0 , 1 ]) predicate = CleanlabReason . from_proba ( probas , y ) DisagreeReason \u00b6 Assign doubt when two scikit-learn models disagree on a prediction. Parameters Name Type Description Default model1 scikit-learn classifier required model2 a different scikit-learn classifier required Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import DisagreeReason X , y = load_iris ( return_X_y = True ) model1 = LogisticRegression ( max_iter = 1_000 ) model2 = KNeighborsClassifier () model1 . fit ( X , y ) model2 . fit ( X , y ) doubt = DoubtEnsemble ( reason = DisagreeReason ( model1 , model2 )) indices = doubt . get_indices ( X , y ) from_pred ( pred1 , pred2 ) (staticmethod) \u00b6 Show source code in doubtlab/reason.py 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 @staticmethod def from_pred ( pred1 , pred2 ): \"\"\" Outputs a reason array from two pred arrays, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import DisagreeReason pred1 = [0, 1, 2] pred2 = [0, 1, 1] predicate = DisagreeReason.from_pred(pred1, pred2) assert np.all(predicate == np.array([0.0, 0.0, 1.0])) ``` \"\"\" return ( np . array ( pred1 ) != np . array ( pred2 )) . astype ( np . float16 ) Outputs a reason array from two pred arrays, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import DisagreeReason pred1 = [ 0 , 1 , 2 ] pred2 = [ 0 , 1 , 1 ] predicate = DisagreeReason . from_pred ( pred1 , pred2 ) assert np . all ( predicate == np . array ([ 0.0 , 0.0 , 1.0 ])) LongConfidenceReason \u00b6 Assign doubt when a wrong class gains too much confidence. Parameters Name Type Description Default model scikit-learn classifier required threshold confidence threshold for doubt assignment 0.2 Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import LongConfidenceReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) doubt = DoubtEnsemble ( reason = LongConfidenceReason ( model = model )) indices = doubt . get_indices ( X , y ) from_proba ( proba , y , classes , threshold ) (staticmethod) \u00b6 Show source code in doubtlab/reason.py 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 @staticmethod def from_proba ( proba , y , classes , threshold ): \"\"\" Outputs a reason array from a proba array, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import LongConfidenceReason probas = np.array([[0.9, 0.1], [0.5, 0.5], [0.2, 0.8]]) y = np.array([0, 1, 0]) classes = np.array([0, 1]) threshold = 0.4 predicate = LongConfidenceReason.from_proba(probas, y, classes, threshold) assert np.all(predicate == np.array([0.0, 1.0, 1.0])) ``` \"\"\" values = [] for i , proba in enumerate ( proba ): proba_dict = { classes [ j ]: v for j , v in enumerate ( proba ) if j != y [ i ]} values . append ( max ( proba_dict . values ())) confidences = np . array ( values ) return ( confidences > threshold ) . astype ( np . float16 ) Outputs a reason array from a proba array, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import LongConfidenceReason probas = np . array ([[ 0.9 , 0.1 ], [ 0.5 , 0.5 ], [ 0.2 , 0.8 ]]) y = np . array ([ 0 , 1 , 0 ]) classes = np . array ([ 0 , 1 ]) threshold = 0.4 predicate = LongConfidenceReason . from_proba ( probas , y , classes , threshold ) assert np . all ( predicate == np . array ([ 0.0 , 1.0 , 1.0 ])) MarginConfidenceReason \u00b6 Assign doubt when the difference between the top two most confident classes is too small. Throws an error when there are only two classes. Parameters Name Type Description Default model scikit-learn classifier required threshold confidence threshold for doubt assignment 0.2 Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import MarginConfidenceReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) doubt = DoubtEnsemble ( reason = MarginConfidenceReason ( model = model )) indices = doubt . get_indices ( X , y ) from_proba ( proba , threshold = 0.2 ) (staticmethod) \u00b6 Show source code in doubtlab/reason.py 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 @staticmethod def from_proba ( proba , threshold = 0.2 ): \"\"\" Outputs a reason array from a proba array, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import MarginConfidenceReason probas = np.array([[0.9, 0.1, 0.0], [0.5, 0.4, 0.1]]) predicate = MarginConfidenceReason.from_proba(probas, threshold=0.3) assert np.all(predicate == np.array([0.0, 1.0])) ``` \"\"\" sorted = np . sort ( proba , axis = 1 ) margin = sorted [:, - 1 ] - sorted [:, - 2 ] return ( margin < threshold ) . astype ( np . float16 ) Outputs a reason array from a proba array, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import MarginConfidenceReason probas = np . array ([[ 0.9 , 0.1 , 0.0 ], [ 0.5 , 0.4 , 0.1 ]]) predicate = MarginConfidenceReason . from_proba ( probas , threshold = 0.3 ) assert np . all ( predicate == np . array ([ 0.0 , 1.0 ])) OutlierReason \u00b6 Assign doubt when a scikit-learn outlier model detects an outlier. Parameters Name Type Description Default model scikit-learn outlier model required Usage: from sklearn.datasets import load_iris from sklearn.ensemble import IsolationForest from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import OutlierReason X , y = load_iris ( return_X_y = True ) model = IsolationForest () model . fit ( X ) doubt = DoubtEnsemble ( reason = OutlierReason ( model )) indices = doubt . get_indices ( X , y ) ProbaReason \u00b6 Assign doubt based on low proba-confidence values from a scikit-learn model. Parameters Name Type Description Default model scikit-learn classifier required max_proba maximum probability threshold for doubt assignment 0.55 Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) doubt = DoubtEnsemble ( reason = ProbaReason ( model , max_proba = 0.55 )) indices = doubt . get_indices ( X , y ) from_proba ( proba , max_proba = 0.55 ) (staticmethod) \u00b6 Show source code in doubtlab/reason.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 @staticmethod def from_proba ( proba , max_proba = 0.55 ): \"\"\" Outputs a reason array from a proba array, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import ProbaReason probas = np.array([[0.9, 0.1], [0.5, 0.5]]) predicate = ProbaReason.from_proba(probas) assert np.all(predicate == np.array([0.0, 1.0])) ``` \"\"\" return ( proba . max ( axis = 1 ) <= max_proba ) . astype ( np . float16 ) Outputs a reason array from a proba array, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import ProbaReason probas = np . array ([[ 0.9 , 0.1 ], [ 0.5 , 0.5 ]]) predicate = ProbaReason . from_proba ( probas ) assert np . all ( predicate == np . array ([ 0.0 , 1.0 ])) RandomReason \u00b6 Assign doubt based on a random value. Parameters Name Type Description Default probability probability of assigning a doubt 0.01 random_seed seed for random number generator 42 Usage: from sklearn.datasets import load_iris from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import RandomReason X , y = load_iris ( return_X_y = True ) doubt = DoubtEnsemble ( reason = RandomReason ( probability = 0.05 , random_seed = 42 )) indices = doubt . get_indices ( X , y ) RelativeDifferenceReason \u00b6 Assign doubt when the relative difference between label and regression is too large. Parameters Name Type Description Default model scikit-learn outlier model required threshold cutoff for doubt assignment required Usage: from sklearn.datasets import load_diabetes from sklearn.linear_model import LinearRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import RelativeDifferenceReason X , y = load_diabetes ( return_X_y = True ) model = LinearRegression () model . fit ( X , y ) doubt = DoubtEnsemble ( reason = RelativeDifferenceReason ( model , threshold = 0.5 )) indices = doubt . get_indices ( X , y ) ShannonEntropyReason \u00b6 Assign doubt when the normalized Shannon entropy is too high, see here for a discussion. Parameters Name Type Description Default model scikit-learn classifier required threshold confidence threshold for doubt assignment 0.5 smoothing constant value added to probas to prevent division by zeor 1e-05 Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ShannonEntropyReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) doubt = DoubtEnsemble ( reason = ShannonEntropyReason ( model = model )) indices = doubt . get_indices ( X , y ) from_proba ( proba , threshold = 0.5 , smoothing = 1e-05 ) (staticmethod) \u00b6 Show source code in doubtlab/reason.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 @staticmethod def from_proba ( proba , threshold = 0.5 , smoothing = 1e-5 ): \"\"\" Outputs a reason array from a prediction array, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import ShannonEntropyReason probas = np.array([[0.9, 0.1, 0.0], [0.5, 0.4, 0.1]]) predicate = ShannonEntropyReason.from_proba(probas, threshold=0.8) assert np.all(predicate == np.array([0.0, 1.0])) ``` \"\"\" probas = proba + smoothing entropies = - ( probas * np . log ( probas ) / np . log ( probas . shape [ 1 ])) . sum ( axis = 1 ) return ( entropies > threshold ) . astype ( np . float16 ) Outputs a reason array from a prediction array, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import ShannonEntropyReason probas = np . array ([[ 0.9 , 0.1 , 0.0 ], [ 0.5 , 0.4 , 0.1 ]]) predicate = ShannonEntropyReason . from_proba ( probas , threshold = 0.8 ) assert np . all ( predicate == np . array ([ 0.0 , 1.0 ])) ShortConfidenceReason \u00b6 Assign doubt when the correct class gains too little confidence. Parameters Name Type Description Default model scikit-learn classifier required threshold confidence threshold for doubt assignment 0.2 Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ShortConfidenceReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) doubt = DoubtEnsemble ( reason = ShortConfidenceReason ( model = model , threshold = 0.4 )) indices = doubt . get_indices ( X , y ) from_proba ( proba , y , classes , threshold = 0.2 ) (staticmethod) \u00b6 Show source code in doubtlab/reason.py 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 @staticmethod def from_proba ( proba , y , classes , threshold = 0.2 ): \"\"\" Outputs a reason array from a proba array, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import ShortConfidenceReason probas = np.array([[0.9, 0.1], [0.5, 0.5], [0.3, 0.7]]) y = np.array([0, 1, 0]) classes = np.array([0, 1]) threshold = 0.4 predicate = ShortConfidenceReason.from_proba(probas, y, classes, threshold) assert np.all(predicate == np.array([0.0, 0.0, 1.0])) ``` \"\"\" values = [] for i , p in enumerate ( proba ): proba_dict = { classes [ j ]: v for j , v in enumerate ( p )} values . append ( proba_dict [ y [ i ]]) confidences = np . array ( values ) return ( confidences < threshold ) . astype ( np . float16 ) Outputs a reason array from a proba array, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import ShortConfidenceReason probas = np . array ([[ 0.9 , 0.1 ], [ 0.5 , 0.5 ], [ 0.3 , 0.7 ]]) y = np . array ([ 0 , 1 , 0 ]) classes = np . array ([ 0 , 1 ]) threshold = 0.4 predicate = ShortConfidenceReason . from_proba ( probas , y , classes , threshold ) assert np . all ( predicate == np . array ([ 0.0 , 0.0 , 1.0 ])) WrongPredictionReason \u00b6 Assign doubt when the model prediction doesn't match the label. Parameters Name Type Description Default model scikit-learn classifier required Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import WrongPredictionReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) doubt = DoubtEnsemble ( reason = WrongPredictionReason ( model = model )) indices = doubt . get_indices ( X , y ) from_predict ( pred , y ) (staticmethod) \u00b6 Show source code in doubtlab/reason.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 @staticmethod def from_predict ( pred , y ): \"\"\" Outputs a reason array from a prediction array, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import WrongPredictionReason preds = np.array([\"positive\", \"negative\"]) y = np.array([\"positive\", \"neutral\"]) predicate = WrongPredictionReason.from_predict(preds, y) assert np.all(predicate == np.array([0.0, 1.0])) ``` \"\"\" return ( pred != y ) . astype ( np . float16 ) Outputs a reason array from a prediction array, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import WrongPredictionReason preds = np . array ([ \"positive\" , \"negative\" ]) y = np . array ([ \"positive\" , \"neutral\" ]) predicate = WrongPredictionReason . from_predict ( preds , y ) assert np . all ( predicate == np . array ([ 0.0 , 1.0 ]))","title":"reasons"},{"location":"api/reasons/#from-doubtlabreason-import","text":"","title":"from doubtlab.reason import *"},{"location":"api/reasons/#doubtlab.reason.AbsoluteDifferenceReason","text":"Assign doubt when the absolute difference between label and regression is too large. Parameters Name Type Description Default model scikit-learn outlier model required threshold cutoff for doubt assignment required Usage: from sklearn.datasets import load_diabetes from sklearn.linear_model import LinearRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import AbsoluteDifferenceReason X , y = load_diabetes ( return_X_y = True ) model = LinearRegression () model . fit ( X , y ) doubt = DoubtEnsemble ( reason = AbsoluteDifferenceReason ( model , threshold = 100 )) indices = doubt . get_indices ( X , y )","title":"AbsoluteDifferenceReason"},{"location":"api/reasons/#doubtlab.reason.CleanlabReason","text":"Assign doubt when using the cleanlab heuristic. Parameters Name Type Description Default model scikit-learn outlier model required sorted_index_method method used by cleanlab for sorting indices 'normalized_margin' min_doubt the minimum doubt output value used for sorting by the ensemble 0.5 Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import CleanlabReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression () model . fit ( X , y ) doubt = DoubtEnsemble ( reason = CleanlabReason ( model )) indices = doubt . get_indices ( X , y )","title":"CleanlabReason"},{"location":"api/reasons/#doubtlab.reason.CleanlabReason.from_proba","text":"Show source code in doubtlab/reason.py 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 @staticmethod def from_proba ( proba , y , min_doubt = 0.5 , sorted_index_method = \"normalized_margin\" ): \"\"\" Outputs a reason array from a proba array, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import CleanlabReason probas = np.array([[0.9, 0.1], [0.5, 0.5]]) y = np.array([0, 1]) predicate = CleanlabReason.from_proba(probas, y) ``` \"\"\" ordered_label_errors = get_noise_indices ( y , proba , sorted_index_method ) result = np . zeros_like ( y ) conf_arr = np . linspace ( 1 , min_doubt , result . shape [ 0 ]) for idx , _ in zip ( ordered_label_errors , conf_arr ): result [ idx ] = 1 return result . astype ( np . float16 ) Outputs a reason array from a proba array, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import CleanlabReason probas = np . array ([[ 0.9 , 0.1 ], [ 0.5 , 0.5 ]]) y = np . array ([ 0 , 1 ]) predicate = CleanlabReason . from_proba ( probas , y )","title":"from_proba()"},{"location":"api/reasons/#doubtlab.reason.DisagreeReason","text":"Assign doubt when two scikit-learn models disagree on a prediction. Parameters Name Type Description Default model1 scikit-learn classifier required model2 a different scikit-learn classifier required Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import DisagreeReason X , y = load_iris ( return_X_y = True ) model1 = LogisticRegression ( max_iter = 1_000 ) model2 = KNeighborsClassifier () model1 . fit ( X , y ) model2 . fit ( X , y ) doubt = DoubtEnsemble ( reason = DisagreeReason ( model1 , model2 )) indices = doubt . get_indices ( X , y )","title":"DisagreeReason"},{"location":"api/reasons/#doubtlab.reason.DisagreeReason.from_pred","text":"Show source code in doubtlab/reason.py 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 @staticmethod def from_pred ( pred1 , pred2 ): \"\"\" Outputs a reason array from two pred arrays, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import DisagreeReason pred1 = [0, 1, 2] pred2 = [0, 1, 1] predicate = DisagreeReason.from_pred(pred1, pred2) assert np.all(predicate == np.array([0.0, 0.0, 1.0])) ``` \"\"\" return ( np . array ( pred1 ) != np . array ( pred2 )) . astype ( np . float16 ) Outputs a reason array from two pred arrays, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import DisagreeReason pred1 = [ 0 , 1 , 2 ] pred2 = [ 0 , 1 , 1 ] predicate = DisagreeReason . from_pred ( pred1 , pred2 ) assert np . all ( predicate == np . array ([ 0.0 , 0.0 , 1.0 ]))","title":"from_pred()"},{"location":"api/reasons/#doubtlab.reason.LongConfidenceReason","text":"Assign doubt when a wrong class gains too much confidence. Parameters Name Type Description Default model scikit-learn classifier required threshold confidence threshold for doubt assignment 0.2 Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import LongConfidenceReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) doubt = DoubtEnsemble ( reason = LongConfidenceReason ( model = model )) indices = doubt . get_indices ( X , y )","title":"LongConfidenceReason"},{"location":"api/reasons/#doubtlab.reason.LongConfidenceReason.from_proba","text":"Show source code in doubtlab/reason.py 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 @staticmethod def from_proba ( proba , y , classes , threshold ): \"\"\" Outputs a reason array from a proba array, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import LongConfidenceReason probas = np.array([[0.9, 0.1], [0.5, 0.5], [0.2, 0.8]]) y = np.array([0, 1, 0]) classes = np.array([0, 1]) threshold = 0.4 predicate = LongConfidenceReason.from_proba(probas, y, classes, threshold) assert np.all(predicate == np.array([0.0, 1.0, 1.0])) ``` \"\"\" values = [] for i , proba in enumerate ( proba ): proba_dict = { classes [ j ]: v for j , v in enumerate ( proba ) if j != y [ i ]} values . append ( max ( proba_dict . values ())) confidences = np . array ( values ) return ( confidences > threshold ) . astype ( np . float16 ) Outputs a reason array from a proba array, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import LongConfidenceReason probas = np . array ([[ 0.9 , 0.1 ], [ 0.5 , 0.5 ], [ 0.2 , 0.8 ]]) y = np . array ([ 0 , 1 , 0 ]) classes = np . array ([ 0 , 1 ]) threshold = 0.4 predicate = LongConfidenceReason . from_proba ( probas , y , classes , threshold ) assert np . all ( predicate == np . array ([ 0.0 , 1.0 , 1.0 ]))","title":"from_proba()"},{"location":"api/reasons/#doubtlab.reason.MarginConfidenceReason","text":"Assign doubt when the difference between the top two most confident classes is too small. Throws an error when there are only two classes. Parameters Name Type Description Default model scikit-learn classifier required threshold confidence threshold for doubt assignment 0.2 Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import MarginConfidenceReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) doubt = DoubtEnsemble ( reason = MarginConfidenceReason ( model = model )) indices = doubt . get_indices ( X , y )","title":"MarginConfidenceReason"},{"location":"api/reasons/#doubtlab.reason.MarginConfidenceReason.from_proba","text":"Show source code in doubtlab/reason.py 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 @staticmethod def from_proba ( proba , threshold = 0.2 ): \"\"\" Outputs a reason array from a proba array, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import MarginConfidenceReason probas = np.array([[0.9, 0.1, 0.0], [0.5, 0.4, 0.1]]) predicate = MarginConfidenceReason.from_proba(probas, threshold=0.3) assert np.all(predicate == np.array([0.0, 1.0])) ``` \"\"\" sorted = np . sort ( proba , axis = 1 ) margin = sorted [:, - 1 ] - sorted [:, - 2 ] return ( margin < threshold ) . astype ( np . float16 ) Outputs a reason array from a proba array, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import MarginConfidenceReason probas = np . array ([[ 0.9 , 0.1 , 0.0 ], [ 0.5 , 0.4 , 0.1 ]]) predicate = MarginConfidenceReason . from_proba ( probas , threshold = 0.3 ) assert np . all ( predicate == np . array ([ 0.0 , 1.0 ]))","title":"from_proba()"},{"location":"api/reasons/#doubtlab.reason.OutlierReason","text":"Assign doubt when a scikit-learn outlier model detects an outlier. Parameters Name Type Description Default model scikit-learn outlier model required Usage: from sklearn.datasets import load_iris from sklearn.ensemble import IsolationForest from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import OutlierReason X , y = load_iris ( return_X_y = True ) model = IsolationForest () model . fit ( X ) doubt = DoubtEnsemble ( reason = OutlierReason ( model )) indices = doubt . get_indices ( X , y )","title":"OutlierReason"},{"location":"api/reasons/#doubtlab.reason.ProbaReason","text":"Assign doubt based on low proba-confidence values from a scikit-learn model. Parameters Name Type Description Default model scikit-learn classifier required max_proba maximum probability threshold for doubt assignment 0.55 Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) doubt = DoubtEnsemble ( reason = ProbaReason ( model , max_proba = 0.55 )) indices = doubt . get_indices ( X , y )","title":"ProbaReason"},{"location":"api/reasons/#doubtlab.reason.ProbaReason.from_proba","text":"Show source code in doubtlab/reason.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 @staticmethod def from_proba ( proba , max_proba = 0.55 ): \"\"\" Outputs a reason array from a proba array, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import ProbaReason probas = np.array([[0.9, 0.1], [0.5, 0.5]]) predicate = ProbaReason.from_proba(probas) assert np.all(predicate == np.array([0.0, 1.0])) ``` \"\"\" return ( proba . max ( axis = 1 ) <= max_proba ) . astype ( np . float16 ) Outputs a reason array from a proba array, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import ProbaReason probas = np . array ([[ 0.9 , 0.1 ], [ 0.5 , 0.5 ]]) predicate = ProbaReason . from_proba ( probas ) assert np . all ( predicate == np . array ([ 0.0 , 1.0 ]))","title":"from_proba()"},{"location":"api/reasons/#doubtlab.reason.RandomReason","text":"Assign doubt based on a random value. Parameters Name Type Description Default probability probability of assigning a doubt 0.01 random_seed seed for random number generator 42 Usage: from sklearn.datasets import load_iris from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import RandomReason X , y = load_iris ( return_X_y = True ) doubt = DoubtEnsemble ( reason = RandomReason ( probability = 0.05 , random_seed = 42 )) indices = doubt . get_indices ( X , y )","title":"RandomReason"},{"location":"api/reasons/#doubtlab.reason.RelativeDifferenceReason","text":"Assign doubt when the relative difference between label and regression is too large. Parameters Name Type Description Default model scikit-learn outlier model required threshold cutoff for doubt assignment required Usage: from sklearn.datasets import load_diabetes from sklearn.linear_model import LinearRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import RelativeDifferenceReason X , y = load_diabetes ( return_X_y = True ) model = LinearRegression () model . fit ( X , y ) doubt = DoubtEnsemble ( reason = RelativeDifferenceReason ( model , threshold = 0.5 )) indices = doubt . get_indices ( X , y )","title":"RelativeDifferenceReason"},{"location":"api/reasons/#doubtlab.reason.ShannonEntropyReason","text":"Assign doubt when the normalized Shannon entropy is too high, see here for a discussion. Parameters Name Type Description Default model scikit-learn classifier required threshold confidence threshold for doubt assignment 0.5 smoothing constant value added to probas to prevent division by zeor 1e-05 Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ShannonEntropyReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) doubt = DoubtEnsemble ( reason = ShannonEntropyReason ( model = model )) indices = doubt . get_indices ( X , y )","title":"ShannonEntropyReason"},{"location":"api/reasons/#doubtlab.reason.ShannonEntropyReason.from_proba","text":"Show source code in doubtlab/reason.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 @staticmethod def from_proba ( proba , threshold = 0.5 , smoothing = 1e-5 ): \"\"\" Outputs a reason array from a prediction array, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import ShannonEntropyReason probas = np.array([[0.9, 0.1, 0.0], [0.5, 0.4, 0.1]]) predicate = ShannonEntropyReason.from_proba(probas, threshold=0.8) assert np.all(predicate == np.array([0.0, 1.0])) ``` \"\"\" probas = proba + smoothing entropies = - ( probas * np . log ( probas ) / np . log ( probas . shape [ 1 ])) . sum ( axis = 1 ) return ( entropies > threshold ) . astype ( np . float16 ) Outputs a reason array from a prediction array, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import ShannonEntropyReason probas = np . array ([[ 0.9 , 0.1 , 0.0 ], [ 0.5 , 0.4 , 0.1 ]]) predicate = ShannonEntropyReason . from_proba ( probas , threshold = 0.8 ) assert np . all ( predicate == np . array ([ 0.0 , 1.0 ]))","title":"from_proba()"},{"location":"api/reasons/#doubtlab.reason.ShortConfidenceReason","text":"Assign doubt when the correct class gains too little confidence. Parameters Name Type Description Default model scikit-learn classifier required threshold confidence threshold for doubt assignment 0.2 Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ShortConfidenceReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) doubt = DoubtEnsemble ( reason = ShortConfidenceReason ( model = model , threshold = 0.4 )) indices = doubt . get_indices ( X , y )","title":"ShortConfidenceReason"},{"location":"api/reasons/#doubtlab.reason.ShortConfidenceReason.from_proba","text":"Show source code in doubtlab/reason.py 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 @staticmethod def from_proba ( proba , y , classes , threshold = 0.2 ): \"\"\" Outputs a reason array from a proba array, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import ShortConfidenceReason probas = np.array([[0.9, 0.1], [0.5, 0.5], [0.3, 0.7]]) y = np.array([0, 1, 0]) classes = np.array([0, 1]) threshold = 0.4 predicate = ShortConfidenceReason.from_proba(probas, y, classes, threshold) assert np.all(predicate == np.array([0.0, 0.0, 1.0])) ``` \"\"\" values = [] for i , p in enumerate ( proba ): proba_dict = { classes [ j ]: v for j , v in enumerate ( p )} values . append ( proba_dict [ y [ i ]]) confidences = np . array ( values ) return ( confidences < threshold ) . astype ( np . float16 ) Outputs a reason array from a proba array, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import ShortConfidenceReason probas = np . array ([[ 0.9 , 0.1 ], [ 0.5 , 0.5 ], [ 0.3 , 0.7 ]]) y = np . array ([ 0 , 1 , 0 ]) classes = np . array ([ 0 , 1 ]) threshold = 0.4 predicate = ShortConfidenceReason . from_proba ( probas , y , classes , threshold ) assert np . all ( predicate == np . array ([ 0.0 , 0.0 , 1.0 ]))","title":"from_proba()"},{"location":"api/reasons/#doubtlab.reason.WrongPredictionReason","text":"Assign doubt when the model prediction doesn't match the label. Parameters Name Type Description Default model scikit-learn classifier required Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import WrongPredictionReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) doubt = DoubtEnsemble ( reason = WrongPredictionReason ( model = model )) indices = doubt . get_indices ( X , y )","title":"WrongPredictionReason"},{"location":"api/reasons/#doubtlab.reason.WrongPredictionReason.from_predict","text":"Show source code in doubtlab/reason.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 @staticmethod def from_predict ( pred , y ): \"\"\" Outputs a reason array from a prediction array, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import WrongPredictionReason preds = np.array([\"positive\", \"negative\"]) y = np.array([\"positive\", \"neutral\"]) predicate = WrongPredictionReason.from_predict(preds, y) assert np.all(predicate == np.array([0.0, 1.0])) ``` \"\"\" return ( pred != y ) . astype ( np . float16 ) Outputs a reason array from a prediction array, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import WrongPredictionReason preds = np . array ([ \"positive\" , \"negative\" ]) y = np . array ([ \"positive\" , \"neutral\" ]) predicate = WrongPredictionReason . from_predict ( preds , y ) assert np . all ( predicate == np . array ([ 0.0 , 1.0 ]))","title":"from_predict()"},{"location":"examples/google-emotions/","text":"This example is based on this blogpost . It is also the example that motivated the creation of this project. Google Emotions \u00b6 We're going to check for bad labels in the Google Emotions dataset. This dataset contains text from Reddit (so expect profanity) with emotion tags attached. There are 28 different tags and a single text can belong to more than one emotion. We'll explore the \"excitement\" emotion here, but the exercise can be repeated for many other emotions too. The dataset comes with a paper that lists details . When you read the paper, you'll observe that a genuine effort was taken to make a high quality dataset. There are 82 raters involved n labelling this dataset. Each example should have been at least 3 people checking it. The paper mentions that all the folks who rated were from India but spoke English natively. An effort was made to remove subreddits that were not safe for work or that contained too much vulgar tokens (according to a predefined word-list). An effort was made to balance different subreddits such that larger subreddits wouldn\u2019t bias the dataset. An effort was made to remove subreddits that didn\u2019t offer a variety of emotions. An effort was made to mask names of people as well as references to religions. An effort was made to, in hindsight, confirm that there is sufficient interrated correlation. Given that this is a dataset from Google , and the fact that there's a paper about it ... how hard would it be to find bad labels? Data Loading \u00b6 Let's load in a portion of the dataset. import pandas as pd df = pd . read_csv ( \"https://github.com/koaning/optimal-on-paper/raw/main/data/goemotions_1.csv\" ) Let's sample a few random rows and zoom in on the excitement column. label_of_interest = 'excitement' ( df [[ 'text' , label_of_interest ]] . loc [ lambda d : d [ label_of_interest ] == 0 ] . sample ( 4 )) This is a sample. text excitement 27233 my favourite singer ([NAME]) helped write one of the songs so i love it 0 1385 No i didn\u2019t all i know is that i binged 3 seasoms of it. 0 17077 I liked [NAME]... 0 55699 A \"wise\" man once told me: > DO > YOUR > OWN >RESEARCH >! 0 Again, we should remind folks that this is reddit data. Beware vulgar language. Models \u00b6 Let's set up two modelling pipelines to detect the emotion. Let's start with a simple CountVectorizer model. from sklearn.pipeline import make_pipeline from sklearn.linear_model import LogisticRegression from sklearn.feature_extraction.text import CountVectorizer X , y = list ( df [ 'text' ]), df [ label_of_interest ] pipe = make_pipeline ( CountVectorizer (), LogisticRegression ( class_weight = 'balanced' , max_iter = 1000 ) ) Next, let's also make a pipeline that uses text embeddings. We'll use the whatlies library to do this. from sklearn.pipeline import make_union from whatlies.language import BytePairLanguage pipe_emb = make_pipeline ( make_union ( BytePairLanguage ( \"en\" , vs = 1_000 ), BytePairLanguage ( \"en\" , vs = 100_000 ) ), LogisticRegression ( class_weight = 'balanced' , max_iter = 1000 ) ) Let's train both pipelines before moving on. pipe . fit ( X , y ) pipe_emb . fit ( X , y ) Assign Doubt \u00b6 Let's now create a doubt ensemble using these two pipelines. from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason , DisagreeReason , ShortConfidenceReason reasons = { 'proba' : ProbaReason ( pipe ), 'disagree' : DisagreeReason ( pipe , pipe_emb ), 'short_pipe' : ShortConfidenceReason ( pipe ), 'short_pipe_emb' : ShortConfidenceReason ( pipe_emb ), } doubt = DoubtEnsemble ( ** reasons ) There are four reasons in this ensemble. proba : This reason will assign doubt when the pipe pipeline doesn't predict any label with a high confidence. disagree : This reason will assign doubt when the pipe pipeline doesn't agree with the pipe_emb pipeline. short_pipe : This reason will assign doubt when the pipe pipeline predicts the correct label with a low confidence. short_pipe_emb : This reason will assign doubt when the pipe_emb pipeline predicts the correct label with a low confidence. All of these reasons have merit to it, but when they overlap we should assign extra attention. The DoubtEnsemble will assign the priority based on overlap on your behalf. Exploring Examples \u00b6 Let's explore some of the labels that deserve attention. # Return a dataframe with reasoning behind sorting predicates = doubt . get_predicates ( X , y ) # Use predicates to sort original dataframe df_sorted = df . iloc [ predicates . index ][[ 'text' , label_of_interest ]] # Create a dataframe containing predicates and original data df_label = pd . concat ([ df_sorted , predicates ], axis = 1 ) Let's check the first few rows of this dataframe. ( df_label [[ 'text' , label_of_interest ]] . head ( 10 )) text excitement Happy Easter everyone!! 0 Happy Easter everyone!! 0 Happy Easter everyone!! 0 Congratulations mate!! 0 Yes every time 0 New flavour! I love it! 0 Wow! Prayers for everyone there. 0 Wow! Prayers for everyone there. 0 Hey vro! 0 Oh my gooooooooood 0 There's some examples that certainly contain excitement. However, these are all examples where the label is 0. Let's re-use this dataframe one more time but now to explore examples where the data says there should be excitement. ( df_label [[ 'text' , label_of_interest ]] . loc [ lambda d : d [ 'excitement' ] == 1 ] . head ( 10 )) text excitement Hate Hate Hate, feels so good. 1 dear... husband 1 The old bear 1 I'd love to do that one day 1 [NAME] damn I love [NAME]. 1 [NAME] is a really cool name! 1 No haha but this is our first day on Reddit! 1 Yeah that pass 1 True! He probably is just lonely. Thank you for the kind words :) 1 a surprise to be sure 1 While some of the examples seem fine, I would argue that \"dear ... husband\" and \"The old bear\" are examples where the label is should be 0. Exploring Reasons \u00b6 It's worth doing a minor deep dive in the behavior behind the different reasons. None of the reasons are perfect, but they all favor different examples for reconsideration. CountVectorizer short on Confidence \u00b6 This is a \"high\"-bias bag-of-words model. It's going to likely overfit on the apperance of a token in the text. ( df_label . sort_values ( \"predicate_short_pipe\" , ascending = False ) . head ( 10 )[[ 'text' , label_of_interest ]] . drop_duplicates ()) text excitement I am inexplicably excited by [NAME]. I get so excited by how he curls passes 0 Omg this is so amazing ! Keep up the awesome work and have a fantastic New Year ! 0 Sounds like a fun game. Our home game around here is .05/.10. Its fun but not very exciting. 0 So no replays for arsenal penalty calls.. Cool cool cool cool cool cool cool cool 0 Wow, your posting history is a real... interesting ride. 0 No different than people making a big deal about their team winning the super bowl. People find it interesting. 0 Hey congrats!! That's amazing, you've done such amazing progress! Hope you have a great day :) 0 I just read your list and now I can't wait, either!! Hurry up with the happy, relieved and peaceful onward and upward!! Congratulations\ud83d\ude0e 0 CountVectorizer with Low Proba \u00b6 This is a \"high\"-bias bag-of-words model when it isn't confident. It's going to likely overfit examples with tokens that appear in both classes. ( df_label . sort_values ( \"predicate_proba\" , ascending = False ) . head ( 10 )[[ 'text' , label_of_interest ]] . drop_duplicates ()) text excitement Happy Easter everyone!! 0 This game is on [NAME]... 0 I swear if it's the Cowboys and the Patriots in the Super Bowl I'm going to burn something down. 0 I'm on red pills :) 0 Wow. I hope that asst manager will be looking for a new job soon. 0 No lie I was just fucking watching the office but I paused it and am know listening to graduation and browsing this subreddit 0 I was imagining her just coming in to work wearing the full [NAME] look. 0 Like this game from a week ago? 26 points 14 0 You should come. You'd enjoy it. 0 I almost pissed myself waiting so long in the tunnel. Not a fun feeling 0 BytePair Embeddings short on Confidence \u00b6 This is model based on just word embeddings. These embeddings are pooled together before being passed to the classifier which is likely why it favors short texts. ( df_label . sort_values ( \"predicate_short_pipe_emb\" , ascending = False ) . head ( 20 )[[ 'text' , label_of_interest ]] . drop_duplicates ()) text excitement Woot woot! 0 WOW!!! 0 Happy birthday! 0 Happy Birthday! 0 Happy one week anniversary 0 Happy Birthday!!! 0 Pop pop! 0 Enjoy the ride! 0 Very interesting!!! 0 My exact reaction 0 happy birthday dude! 0 Enjoy 0 Oh wow!!! 0 This sounds interesting 0 Conclusion \u00b6 This example demonstrates two things. By combining reasons into an ensemble, we get a pretty good system to spot examples worth double checking. It's fairly easy to find bad labels, even in a dataset hosted by Google, even when there's an article written about it. This does not bode well for any models trained on this dataset. Required Nuance \u00b6 We think this example demonstrates the utility of doubtlab and that it also serves as a useful case-study that warns people of the dangers of label errors. That said, we want to mention a few points of nuance. The emotions dataset also comes with a column for the rater_id and example_very_unclear . Some of the examples that we've found using doubtlab also have disagreement between raters. The unclear-example flag is also raised a few times when we spot a bad label. One can only commend the authors for taking this effort because these columns help explain that some of the labels shouldn't be taken at face value. It also deserves mentioning that emotion detection is genuinely an incredibly hard task to label. There's so much context and culture involved in expressing emotion in a natural language that I cannot expect a \"pure label\" to even exist. Sarcasm detection is an unsolved problem. If sarcasm is unsolved, how on earth can we guarantee emotion detection or sentiment? Next Steps \u00b6 Feel free to repeat this exercise but with a different emotion or with different reasoning in the ensemble.","title":"Google Emotions"},{"location":"examples/google-emotions/#google-emotions","text":"We're going to check for bad labels in the Google Emotions dataset. This dataset contains text from Reddit (so expect profanity) with emotion tags attached. There are 28 different tags and a single text can belong to more than one emotion. We'll explore the \"excitement\" emotion here, but the exercise can be repeated for many other emotions too. The dataset comes with a paper that lists details . When you read the paper, you'll observe that a genuine effort was taken to make a high quality dataset. There are 82 raters involved n labelling this dataset. Each example should have been at least 3 people checking it. The paper mentions that all the folks who rated were from India but spoke English natively. An effort was made to remove subreddits that were not safe for work or that contained too much vulgar tokens (according to a predefined word-list). An effort was made to balance different subreddits such that larger subreddits wouldn\u2019t bias the dataset. An effort was made to remove subreddits that didn\u2019t offer a variety of emotions. An effort was made to mask names of people as well as references to religions. An effort was made to, in hindsight, confirm that there is sufficient interrated correlation. Given that this is a dataset from Google , and the fact that there's a paper about it ... how hard would it be to find bad labels?","title":"Google Emotions"},{"location":"examples/google-emotions/#data-loading","text":"Let's load in a portion of the dataset. import pandas as pd df = pd . read_csv ( \"https://github.com/koaning/optimal-on-paper/raw/main/data/goemotions_1.csv\" ) Let's sample a few random rows and zoom in on the excitement column. label_of_interest = 'excitement' ( df [[ 'text' , label_of_interest ]] . loc [ lambda d : d [ label_of_interest ] == 0 ] . sample ( 4 )) This is a sample. text excitement 27233 my favourite singer ([NAME]) helped write one of the songs so i love it 0 1385 No i didn\u2019t all i know is that i binged 3 seasoms of it. 0 17077 I liked [NAME]... 0 55699 A \"wise\" man once told me: > DO > YOUR > OWN >RESEARCH >! 0 Again, we should remind folks that this is reddit data. Beware vulgar language.","title":"Data Loading"},{"location":"examples/google-emotions/#models","text":"Let's set up two modelling pipelines to detect the emotion. Let's start with a simple CountVectorizer model. from sklearn.pipeline import make_pipeline from sklearn.linear_model import LogisticRegression from sklearn.feature_extraction.text import CountVectorizer X , y = list ( df [ 'text' ]), df [ label_of_interest ] pipe = make_pipeline ( CountVectorizer (), LogisticRegression ( class_weight = 'balanced' , max_iter = 1000 ) ) Next, let's also make a pipeline that uses text embeddings. We'll use the whatlies library to do this. from sklearn.pipeline import make_union from whatlies.language import BytePairLanguage pipe_emb = make_pipeline ( make_union ( BytePairLanguage ( \"en\" , vs = 1_000 ), BytePairLanguage ( \"en\" , vs = 100_000 ) ), LogisticRegression ( class_weight = 'balanced' , max_iter = 1000 ) ) Let's train both pipelines before moving on. pipe . fit ( X , y ) pipe_emb . fit ( X , y )","title":"Models"},{"location":"examples/google-emotions/#assign-doubt","text":"Let's now create a doubt ensemble using these two pipelines. from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason , DisagreeReason , ShortConfidenceReason reasons = { 'proba' : ProbaReason ( pipe ), 'disagree' : DisagreeReason ( pipe , pipe_emb ), 'short_pipe' : ShortConfidenceReason ( pipe ), 'short_pipe_emb' : ShortConfidenceReason ( pipe_emb ), } doubt = DoubtEnsemble ( ** reasons ) There are four reasons in this ensemble. proba : This reason will assign doubt when the pipe pipeline doesn't predict any label with a high confidence. disagree : This reason will assign doubt when the pipe pipeline doesn't agree with the pipe_emb pipeline. short_pipe : This reason will assign doubt when the pipe pipeline predicts the correct label with a low confidence. short_pipe_emb : This reason will assign doubt when the pipe_emb pipeline predicts the correct label with a low confidence. All of these reasons have merit to it, but when they overlap we should assign extra attention. The DoubtEnsemble will assign the priority based on overlap on your behalf.","title":"Assign Doubt"},{"location":"examples/google-emotions/#exploring-examples","text":"Let's explore some of the labels that deserve attention. # Return a dataframe with reasoning behind sorting predicates = doubt . get_predicates ( X , y ) # Use predicates to sort original dataframe df_sorted = df . iloc [ predicates . index ][[ 'text' , label_of_interest ]] # Create a dataframe containing predicates and original data df_label = pd . concat ([ df_sorted , predicates ], axis = 1 ) Let's check the first few rows of this dataframe. ( df_label [[ 'text' , label_of_interest ]] . head ( 10 )) text excitement Happy Easter everyone!! 0 Happy Easter everyone!! 0 Happy Easter everyone!! 0 Congratulations mate!! 0 Yes every time 0 New flavour! I love it! 0 Wow! Prayers for everyone there. 0 Wow! Prayers for everyone there. 0 Hey vro! 0 Oh my gooooooooood 0 There's some examples that certainly contain excitement. However, these are all examples where the label is 0. Let's re-use this dataframe one more time but now to explore examples where the data says there should be excitement. ( df_label [[ 'text' , label_of_interest ]] . loc [ lambda d : d [ 'excitement' ] == 1 ] . head ( 10 )) text excitement Hate Hate Hate, feels so good. 1 dear... husband 1 The old bear 1 I'd love to do that one day 1 [NAME] damn I love [NAME]. 1 [NAME] is a really cool name! 1 No haha but this is our first day on Reddit! 1 Yeah that pass 1 True! He probably is just lonely. Thank you for the kind words :) 1 a surprise to be sure 1 While some of the examples seem fine, I would argue that \"dear ... husband\" and \"The old bear\" are examples where the label is should be 0.","title":"Exploring Examples"},{"location":"examples/google-emotions/#exploring-reasons","text":"It's worth doing a minor deep dive in the behavior behind the different reasons. None of the reasons are perfect, but they all favor different examples for reconsideration.","title":"Exploring Reasons"},{"location":"examples/google-emotions/#countvectorizer-short-on-confidence","text":"This is a \"high\"-bias bag-of-words model. It's going to likely overfit on the apperance of a token in the text. ( df_label . sort_values ( \"predicate_short_pipe\" , ascending = False ) . head ( 10 )[[ 'text' , label_of_interest ]] . drop_duplicates ()) text excitement I am inexplicably excited by [NAME]. I get so excited by how he curls passes 0 Omg this is so amazing ! Keep up the awesome work and have a fantastic New Year ! 0 Sounds like a fun game. Our home game around here is .05/.10. Its fun but not very exciting. 0 So no replays for arsenal penalty calls.. Cool cool cool cool cool cool cool cool 0 Wow, your posting history is a real... interesting ride. 0 No different than people making a big deal about their team winning the super bowl. People find it interesting. 0 Hey congrats!! That's amazing, you've done such amazing progress! Hope you have a great day :) 0 I just read your list and now I can't wait, either!! Hurry up with the happy, relieved and peaceful onward and upward!! Congratulations\ud83d\ude0e 0","title":"CountVectorizer short on Confidence"},{"location":"examples/google-emotions/#countvectorizer-with-low-proba","text":"This is a \"high\"-bias bag-of-words model when it isn't confident. It's going to likely overfit examples with tokens that appear in both classes. ( df_label . sort_values ( \"predicate_proba\" , ascending = False ) . head ( 10 )[[ 'text' , label_of_interest ]] . drop_duplicates ()) text excitement Happy Easter everyone!! 0 This game is on [NAME]... 0 I swear if it's the Cowboys and the Patriots in the Super Bowl I'm going to burn something down. 0 I'm on red pills :) 0 Wow. I hope that asst manager will be looking for a new job soon. 0 No lie I was just fucking watching the office but I paused it and am know listening to graduation and browsing this subreddit 0 I was imagining her just coming in to work wearing the full [NAME] look. 0 Like this game from a week ago? 26 points 14 0 You should come. You'd enjoy it. 0 I almost pissed myself waiting so long in the tunnel. Not a fun feeling 0","title":"CountVectorizer with Low Proba"},{"location":"examples/google-emotions/#bytepair-embeddings-short-on-confidence","text":"This is model based on just word embeddings. These embeddings are pooled together before being passed to the classifier which is likely why it favors short texts. ( df_label . sort_values ( \"predicate_short_pipe_emb\" , ascending = False ) . head ( 20 )[[ 'text' , label_of_interest ]] . drop_duplicates ()) text excitement Woot woot! 0 WOW!!! 0 Happy birthday! 0 Happy Birthday! 0 Happy one week anniversary 0 Happy Birthday!!! 0 Pop pop! 0 Enjoy the ride! 0 Very interesting!!! 0 My exact reaction 0 happy birthday dude! 0 Enjoy 0 Oh wow!!! 0 This sounds interesting 0","title":"BytePair Embeddings short on Confidence"},{"location":"examples/google-emotions/#conclusion","text":"This example demonstrates two things. By combining reasons into an ensemble, we get a pretty good system to spot examples worth double checking. It's fairly easy to find bad labels, even in a dataset hosted by Google, even when there's an article written about it. This does not bode well for any models trained on this dataset.","title":"Conclusion"},{"location":"examples/google-emotions/#required-nuance","text":"We think this example demonstrates the utility of doubtlab and that it also serves as a useful case-study that warns people of the dangers of label errors. That said, we want to mention a few points of nuance. The emotions dataset also comes with a column for the rater_id and example_very_unclear . Some of the examples that we've found using doubtlab also have disagreement between raters. The unclear-example flag is also raised a few times when we spot a bad label. One can only commend the authors for taking this effort because these columns help explain that some of the labels shouldn't be taken at face value. It also deserves mentioning that emotion detection is genuinely an incredibly hard task to label. There's so much context and culture involved in expressing emotion in a natural language that I cannot expect a \"pure label\" to even exist. Sarcasm detection is an unsolved problem. If sarcasm is unsolved, how on earth can we guarantee emotion detection or sentiment?","title":"Required Nuance"},{"location":"examples/google-emotions/#next-steps","text":"Feel free to repeat this exercise but with a different emotion or with different reasoning in the ensemble.","title":"Next Steps"},{"location":"quickstart/","text":"The goal of this document is to explain how the library works on a high-level. Datasets and Models \u00b6 You can use doubtlab to check your own datasets for bad labels. Many of the methods that we provide are based on the interaction between a dataset and a model trained on that dataset. For example; from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) This examples shows a logistic regression model trained on the load_iris dataset. The model is able to make predictions on the dataset and it's also able to output a confidence score via model.predict_proba(X) . You could wonder. What might it mean if the confidence values are low? What might it mean if our model cannot make an accurate prediction on a datapoint that it's trained on? In both of these cases, it could be that nothing is wrong. But you could argue that these datapoints may be worth double-checking. Pipeline of Doubt Reasons \u00b6 The doubtlab library allows you to define \"reasons\" to doubt the validity of a datapoint. The code below shows you how to build an ensemble of the two aforementioned reasons. from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason , WrongPredictionReason # Define the reasons with a name. reasons = { \"proba\" : ProbaReason ( model = model , max_proba = 0.55 ), \"wrong_pred\" : WrongPredictionReason ( model = model ), } # Put all the reasons into an ensemble doubt = DoubtEnsemble ( ** reasons ) This ensemble represents a pipeline of reasons to doubt the validity of a label. Internal Details \u00b6 A DoubtEnsemble , technically, is just an ensemble of callables. You could also choose to use lambda functions to define a reason for doubt. The example below shows an example of a lambda function that's equivalent to what WrongPredictionReason would do. DoubtEnsemble ( wrong_pred = lambda X , y : ( model . predict ( X ) != y ) . astype ( float16 ) ) When it's time to infer doubt, the DoubtEnsemble will call each callable reason in order, passing X , y and listening for an array that contains \"doubt-scores\". These scores are just numbers, but they follow a few rules. when there is no doubt, the score should be zero the maximum doubt that a reason can emit is one the higher the doubt-score, the more likely doubt should be Retreiving Examples to Check \u00b6 There are multiple ways of retreiving the examples to check from the doubt pipeline. Get Indices \u00b6 You could simply use the DoubtEnsemble.get_indices method to get the indices of the original data that are in doubt. # Get the ordered indices of examples worth checking again indices = doubt . get_indices ( X , y ) In this case, you'd get an array with 6 elements. array([ 77, 106, 126, 133, 83, 119, 70]) You can inspect the associated rows/labels of the examples via: X [ indices ], y [ indices ] Get Predicates \u00b6 While the indices are useful they don't tell you much about how the ordering took place. If you'd like to see more details, you can also retreive a dataframe with predicates that explain which rows triggered which reasons. # Get the predicates, or reasoning, behind the order predicates = doubt . get_predicates ( X , y ) The predicates dataframe contains a column for each reason. The index refers to the row number in the original dataset. Let's check the top 10 rows. predicates . head ( 10 ) predicate_proba predicate_wrong_pred 77 1 1 106 1 1 126 1 0 133 1 0 83 0 1 119 1 0 70 0 1 105 0 0 107 0 0 104 0 0 There's a few things to observe here. The ensemble assumes that overlap between reasons matter is a reason to give a row priority, moving it up in the dataframe. The .get_indices method tells you what deserves checking and only returns candidates worth checking. The .get_predicates method tries to explain why these rows deserve to be checked and therefore returns a dataframe with a row for each row in X . The index of the predicates dataframe refers to rows in our original X , y arrays. Why do this exercise? \u00b6 It's bad enough to have bad labels in your training data, but if you have bad labels in your validation then it's really game over for your machine learning models. There's ample evidence that many pre-trained academic models have suffered from this problem. So there's a legitimate concern that it may be a problem for your dataset as well. The hope is that this library makes it just a bit easier for folks do to check their datasets for bad labels. It's an exercise worth doing and the author of this library would love to hear anekdotes. Next Steps \u00b6 You may get some more inspiration by checking some of the examples of this library. Once you're ready to give the library a spin we encourage you to explore the suite of reasons that this library supports. General Reasons \u00b6 RandomReason : assign doubt randomly, just for sure OutlierReason : assign doubt when the model declares a row an outlier Classification Reasons \u00b6 ProbaReason : assign doubt when a models' confidence-values are low for any label WrongPredictionReason : assign doubt when a model cannot predict the listed label ShortConfidenceReason : assign doubt when the correct label gains too little confidence LongConfidenceReason : assign doubt when a wrong label gains too much confidence MarginConfidenceReason : assign doubt when there's a small difference between the top two class confidences DisagreeReason : assign doubt when two models disagree on a prediction CleanlabReason : assign doubt according to cleanlab Regression Reasons \u00b6 AbsoluteDifferenceReason : assign doubt when the absolute difference is too high RelativeDifferenceReason : assign doubt when the relative difference is too high If you think there's a reason missing, feel free to mention it on GitHub .","title":"Quickstart"},{"location":"quickstart/#datasets-and-models","text":"You can use doubtlab to check your own datasets for bad labels. Many of the methods that we provide are based on the interaction between a dataset and a model trained on that dataset. For example; from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) This examples shows a logistic regression model trained on the load_iris dataset. The model is able to make predictions on the dataset and it's also able to output a confidence score via model.predict_proba(X) . You could wonder. What might it mean if the confidence values are low? What might it mean if our model cannot make an accurate prediction on a datapoint that it's trained on? In both of these cases, it could be that nothing is wrong. But you could argue that these datapoints may be worth double-checking.","title":"Datasets and Models"},{"location":"quickstart/#pipeline-of-doubt-reasons","text":"The doubtlab library allows you to define \"reasons\" to doubt the validity of a datapoint. The code below shows you how to build an ensemble of the two aforementioned reasons. from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason , WrongPredictionReason # Define the reasons with a name. reasons = { \"proba\" : ProbaReason ( model = model , max_proba = 0.55 ), \"wrong_pred\" : WrongPredictionReason ( model = model ), } # Put all the reasons into an ensemble doubt = DoubtEnsemble ( ** reasons ) This ensemble represents a pipeline of reasons to doubt the validity of a label.","title":"Pipeline of Doubt Reasons"},{"location":"quickstart/#internal-details","text":"A DoubtEnsemble , technically, is just an ensemble of callables. You could also choose to use lambda functions to define a reason for doubt. The example below shows an example of a lambda function that's equivalent to what WrongPredictionReason would do. DoubtEnsemble ( wrong_pred = lambda X , y : ( model . predict ( X ) != y ) . astype ( float16 ) ) When it's time to infer doubt, the DoubtEnsemble will call each callable reason in order, passing X , y and listening for an array that contains \"doubt-scores\". These scores are just numbers, but they follow a few rules. when there is no doubt, the score should be zero the maximum doubt that a reason can emit is one the higher the doubt-score, the more likely doubt should be","title":"Internal Details"},{"location":"quickstart/#retreiving-examples-to-check","text":"There are multiple ways of retreiving the examples to check from the doubt pipeline.","title":"Retreiving Examples to Check"},{"location":"quickstart/#get-indices","text":"You could simply use the DoubtEnsemble.get_indices method to get the indices of the original data that are in doubt. # Get the ordered indices of examples worth checking again indices = doubt . get_indices ( X , y ) In this case, you'd get an array with 6 elements. array([ 77, 106, 126, 133, 83, 119, 70]) You can inspect the associated rows/labels of the examples via: X [ indices ], y [ indices ]","title":"Get Indices"},{"location":"quickstart/#get-predicates","text":"While the indices are useful they don't tell you much about how the ordering took place. If you'd like to see more details, you can also retreive a dataframe with predicates that explain which rows triggered which reasons. # Get the predicates, or reasoning, behind the order predicates = doubt . get_predicates ( X , y ) The predicates dataframe contains a column for each reason. The index refers to the row number in the original dataset. Let's check the top 10 rows. predicates . head ( 10 ) predicate_proba predicate_wrong_pred 77 1 1 106 1 1 126 1 0 133 1 0 83 0 1 119 1 0 70 0 1 105 0 0 107 0 0 104 0 0 There's a few things to observe here. The ensemble assumes that overlap between reasons matter is a reason to give a row priority, moving it up in the dataframe. The .get_indices method tells you what deserves checking and only returns candidates worth checking. The .get_predicates method tries to explain why these rows deserve to be checked and therefore returns a dataframe with a row for each row in X . The index of the predicates dataframe refers to rows in our original X , y arrays.","title":"Get Predicates"},{"location":"quickstart/#why-do-this-exercise","text":"It's bad enough to have bad labels in your training data, but if you have bad labels in your validation then it's really game over for your machine learning models. There's ample evidence that many pre-trained academic models have suffered from this problem. So there's a legitimate concern that it may be a problem for your dataset as well. The hope is that this library makes it just a bit easier for folks do to check their datasets for bad labels. It's an exercise worth doing and the author of this library would love to hear anekdotes.","title":"Why do this exercise?"},{"location":"quickstart/#next-steps","text":"You may get some more inspiration by checking some of the examples of this library. Once you're ready to give the library a spin we encourage you to explore the suite of reasons that this library supports.","title":"Next Steps"},{"location":"quickstart/#general-reasons","text":"RandomReason : assign doubt randomly, just for sure OutlierReason : assign doubt when the model declares a row an outlier","title":"General Reasons"},{"location":"quickstart/#classification-reasons","text":"ProbaReason : assign doubt when a models' confidence-values are low for any label WrongPredictionReason : assign doubt when a model cannot predict the listed label ShortConfidenceReason : assign doubt when the correct label gains too little confidence LongConfidenceReason : assign doubt when a wrong label gains too much confidence MarginConfidenceReason : assign doubt when there's a small difference between the top two class confidences DisagreeReason : assign doubt when two models disagree on a prediction CleanlabReason : assign doubt according to cleanlab","title":"Classification Reasons"},{"location":"quickstart/#regression-reasons","text":"AbsoluteDifferenceReason : assign doubt when the absolute difference is too high RelativeDifferenceReason : assign doubt when the relative difference is too high If you think there's a reason missing, feel free to mention it on GitHub .","title":"Regression Reasons"}]}